{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Learning is parameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.1 A hot problem ###\n",
    "import torch\n",
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.2 Choosing a linear model as a first try ###\n",
    "# t_c = w * t_u + b\n",
    "# name w and b after weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.3 Less loss is what you want ###\n",
    "# two loss functions |t_p - t_c| and (t_p - t_c)^2\n",
    "# the square of differences behaves more nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.4 From problem to PyTorch ###\n",
    "# The model\n",
    "def model(t_u, w, b):\n",
    "    return w*t_u + b\n",
    "# Loss function\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p-t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "#  initialize the parameters\n",
    "w = torch.ones(1) # Scalar, the product operation will use broadcasting. \n",
    "b = torch.zeros(1)\n",
    "\n",
    "#  invoke the model\n",
    "t_p = model(t_u, w, b)\n",
    "print('t_p: ', t_p)\n",
    "\n",
    "# Loss\n",
    "loss = loss_fn(t_p, t_c)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.5 Down along the gradient ###\n",
    "# Basic gradient descent: grad = [loss(w+delt)-loss(w-delt)]/2*delt\n",
    "delt = 0.1\n",
    "# w\n",
    "grad_w = (loss_fn(model(t_u, w+delt, b), t_c)-loss_fn(model(t_u, w-delt, b), t_c))/2*delt\n",
    "# b\n",
    "grad_b = (loss_fn(model(t_u, w, b+delt), t_c) - loss_fn(model(t_u, w, b-delt), t_c))/2*delt\n",
    "# learning rate\n",
    "learning_rate = 1e-2\n",
    "w = w-learning_rate*grad_w\n",
    "b = b-learning_rate*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.6 Getting analytical ###\n",
    "# apply the chain rule and compute the derivative of the loss.\n",
    "# d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w)\n",
    "\n",
    "# d loss_fn / d t_p\n",
    "def dloss_fn(t_p, t_c):\n",
    "    return 2*(t_p-t_c)\n",
    "\n",
    "# d t_p / d w\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "# d t_p / d b\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0\n",
    "\n",
    "# grad function.\n",
    "# grad_loss = (grad_w, grad_b)\n",
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    grad_w = dloss_fn(t_p, t_c)*dmodel_dw(t_u, w, b)\n",
    "    grad_b = dloss_fn(t_p, t_c)*dmodel_db(t_u, w, b)\n",
    "    return torch.stack([grad_w.mean(), grad_b.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1.7 The training loop ###\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        w, b = params\n",
    "        # Forward pass\n",
    "        t_p = model(t_u, w, b)\n",
    "        # Backward pass\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        # Update params\n",
    "        params = params - grad*learning_rate\n",
    "        # print\n",
    "        print('epoch: %d, loss: %f' %(epoch, float(loss)))\n",
    "        print('params: ', params)\n",
    "        print('grad: ', grad)\n",
    "    # return params.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now invoke your training loop.\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c\n",
    ")\n",
    "# Blow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set lower learning_rate: 1e-4\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalized input\n",
    "# learning rate states 1e-2\n",
    "t_un = t_u*0.1\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un, # t_un\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change n_epochs to 5000\n",
    "# learning rate states 1e-2\n",
    "t_un = t_u*0.1\n",
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un, # t_un\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your data\n",
    "# https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# out\n",
    "t_p = model(t_un, *params)\n",
    "# plot\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel('Fahrenheit')\n",
    "plt.ylabel('Celsius')\n",
    "# Draw fitted line.\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o') # 'o': circle marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 PyTorch's autograd: Backpropagate all things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Celsius\n",
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "# Fahrenheit\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "\n",
    "# The model\n",
    "def model(t_u, w, b):\n",
    "    return w*t_u + b\n",
    "# Loss function\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p-t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "# Parameters.\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "print('params.grad: ', params.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto grad.\n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "# Backward.\n",
    "loss.backward()\n",
    "print('params.grad: ', params.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd training loop\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Clear grad before call `loss.backward()`\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        # Forward pass\n",
    "        t_p = model(t_u, *params)\n",
    "        # Backward pass\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        # Update params\n",
    "        # detatch(): detatch from computation graph, backpropagate only your current params.\n",
    "        # requires_grad_(): Tracking params, autograd again.\n",
    "        params = (params-learning_rate*params.grad).detach().requires_grad_()\n",
    "        \n",
    "        # print\n",
    "        print('epoch: %d, loss: %f' %(epoch, float(loss)))\n",
    "        print('params: ', params)\n",
    "        print('grad: ', params.grad)\n",
    "    # return params.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normlization.\n",
    "t_un = t_u*0.1\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2.1 Optimizers a la carte ###\n",
    "# Optimization list\n",
    "import torch.optim as optim\n",
    "dir(optim)\n",
    "\n",
    "# use optimizer.\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "# Forward.\n",
    "t_p = model(t_un, *params)\n",
    "# Loss\n",
    "loss = loss_fn(t_p, t_c)\n",
    "# Clear grad.\n",
    "optimizer.zero_grad()\n",
    "# Backward\n",
    "loss.backward()\n",
    "# Update params.\n",
    "optimizer.step()\n",
    "print('params: ', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with optimizer.\n",
    "# Autograd training loop\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        # Forward pass\n",
    "        t_p = model(t_u, *params)\n",
    "        # Backward pass\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        # Clear grad before call `loss.backward()`\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print\n",
    "        if (epoch % 500 == 0):\n",
    "            print('epoch: %d, loss: %f' %(epoch, float(loss)))\n",
    "            print('params: ', params)\n",
    "    # return params.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Normlization.\n",
    "t_un = t_u*0.1\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use adam optimizer\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# increase the learning rate to 1e-1, Adam won't even blink.\n",
    "learning_rate = 1e-1 \n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "# No need normalization\n",
    "training_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u,  # No need normalization\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2.2 Training, validation, and overfitting ###\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2*n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val] # Before the last n_val samples.\n",
    "val_indices = shuffled_indices[-n_val:] # The last n_val samples. \n",
    "\n",
    "# Get train & val set.\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "# Rewrite train loop.\n",
    "def train_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Forward.\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        # Loss\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        # Clear grad.\n",
    "        optimizer.zero_grad()\n",
    "        # Backward\n",
    "        train_loss.backward()\n",
    "        # update params.\n",
    "        optimizer.step()\n",
    "\n",
    "        # print\n",
    "        if (epoch % 500 == 0):\n",
    "            print('epoch: %d, train_loss: %f, val_loss: %f' %(epoch, train_loss, val_loss))\n",
    "    return params\n",
    "\n",
    "# Use adam optimizer\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# increase the learning rate to 1e-1, Adam won't even blink.\n",
    "learning_rate = 1e-1 \n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "# No need normalization\n",
    "train_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_u,  # No need normalization\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_u = val_t_u,\n",
    "    val_t_c = val_t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2.3 Nits in autograd and switching it off ###\n",
    "\n",
    "# Switch off autograd when validation.\n",
    "def calc_forward(t_u, t_c, params, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss\n",
    "\n",
    "# Rewrite train loop\n",
    "def train_loop_plus(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Forward.\n",
    "        train_loss = calc_forward(train_t_u, train_t_c, params, True)\n",
    "        val_loss = calc_forward(val_t_u, val_t_c, params, False)\n",
    "        # Clear grad.\n",
    "        optimizer.zero_grad()\n",
    "        # Backward\n",
    "        train_loss.backward()\n",
    "        # update params.\n",
    "        optimizer.step()\n",
    "\n",
    "        # print\n",
    "        if (epoch % 500 == 0):\n",
    "            print('epoch: %d, train_loss: %f, val_loss: %f' %(epoch, train_loss, val_loss))\n",
    "    return params\n",
    "\n",
    "# No need normalization\n",
    "train_loop_plus(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_u,  # No need normalization\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_u = val_t_u,\n",
    "    val_t_c = val_t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Redefine the model to be w2 * t_u ** 2 + w1 * t_u + b\n",
    "def model_2rd(t_u, w1, w2, b):\n",
    "    return w2*(t_u**2) + w1*t_u + b\n",
    "# Switch off autograd when validation.\n",
    "def calc_forward(t_u, t_c, params, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model_2rd(t_u, *params)  # replace with `model_2rd`\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss\n",
    "\n",
    "# Use adam optimizer\n",
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "# increase the learning rate to 1e-1, Adam won't even blink.\n",
    "learning_rate = 1e-1 \n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "# No need normalization\n",
    "params = train_loop_plus(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_u,  # No need normalization\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_u = val_t_u,\n",
    "    val_t_c = val_t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# Create plot\n",
    "plt.figure(dpi=600)\n",
    "plt.xlabel('Fahrenheit')\n",
    "plt.ylabel('Celsius')\n",
    "\n",
    "# Data\n",
    "t_p = model_2rd(t_u, *params)\n",
    "# plt fitted model.\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "# plt raw data\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
